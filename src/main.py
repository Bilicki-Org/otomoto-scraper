import logging
import pandas as pd
import os
from scraping.otomoto_client import OtomotoClient
from scraping.link_extractor import LinkExtractor
from scraping.offer_parser import OfferParser

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def main():
    logging.info("üöÄ Starting ETL process...")
    
    client = OtomotoClient()
    extractor = LinkExtractor(client)
    
    # KROK 1: Pobieranie link√≥w
    # Ustawiamy 2 strony = ok. 64 og≈Çoszenia. Idealne do test√≥w.
    links = extractor.get_links(start_page=1, num_pages=1)
    
    logging.info(f"üîó Successfully extracted {len(links)} links.")
    
    # KROK 2: Pobieranie szczeg√≥≈Ç√≥w
    parser = OfferParser(client)
    dataset = []
    
    # Pƒôtla po linkach
    for i, link in enumerate(links):
        logging.info(f"Parsing {i+1}/{len(links)}: {link}")
        try:
            data = parser.parse_offer(link)
            if data:
                dataset.append(data)
        except Exception as e:
            logging.error(f"B≈ÇƒÖd przy parsowaniu {link}: {e}")
            
    # KROK 3: Zapis danych
    if dataset:
        output_dir = "data"
        os.makedirs(output_dir, exist_ok=True)
        
        # To jest plik, kt√≥rego Ci brakowa≈Ço:
        output_file = os.path.join(output_dir, "cars_dataset_100.csv")
        
        df = pd.DataFrame(dataset)
        df.to_csv(output_file, index=False)
        logging.info(f"‚úÖ Data saved to {output_file}. Total records: {len(df)}")
    else:
        logging.warning("‚ö†Ô∏è No data extracted.")

if __name__ == "__main__":
    main()